# Mini Chat com LLM

Uma aplicaÃ§Ã£o de chat simples que conecta frontend web com LLMs atravÃ©s de API.

## ğŸš€ Funcionalidades

- **Interface de Chat**: Campo de texto, botÃ£o enviar, histÃ³rico de mensagens
- **IntegraÃ§Ã£o LLM**: Suporte para Gemini
- **Estado de Loading**: Indicador visual durante processamento
- **Tratamento de Erro**: Feedback para falhas de API, timeout e erros do backend

## ğŸ› ï¸ Tecnologias

- **Next.js 14** com App Router
- **TypeScript** para type safety
- **Tailwind CSS** para styling
- **API Routes** para backend
- **Lucide React** para Ã­cones

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clonar e instalar dependÃªncias

```bash
cd chat-feat-llm
npm install
```

### 2. Configurar variÃ¡veis de ambiente

```bash
cp .env.example .env.local
```

Edite o arquivo `.env.local` e configure suas chaves de API:

```bash
# Para Gemini
OPENAI_API_KEY=
LLM_PROVIDER=gemini
GEMINI_API_KEY=your-api-key-here

```

### 3. Executar aplicaÃ§Ã£o

```bash
# Desenvolvimento
npm run dev

# Build para produÃ§Ã£o
npm run build
npm start
```

Acesse: http://localhost:3000

## ğŸ—ï¸ Estrutura do Projeto

```
chat-feat-llm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ chat/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts          # Endpoint POST /chat
â”‚   â”‚   â”œâ”€â”€ globals.css               # Estilos globais
â”‚   â”‚   â”œâ”€â”€ layout.tsx               # Layout principal
â”‚   â”‚   â””â”€â”€ page.tsx                 # PÃ¡gina inicial
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx        # Interface principal do chat
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ llm-providers.ts         # ConfiguraÃ§Ã£o dos provedores LLM
â”œâ”€â”€ .env.example                     # Exemplo de configuraÃ§Ã£o
â”œâ”€â”€ package.json
â”œâ”€â”€ tailwind.config.js
â””â”€â”€ README.md
```

## ğŸ“¡ API Endpoints

### POST /api/chat

Envia mensagem para LLM e retorna resposta.

**Request:**
```json
{
  "message": "OlÃ¡, como vocÃª estÃ¡?"
}
```

**Response:**
```json
{
  "reply": "OlÃ¡! Estou bem, obrigado por perguntar. Como posso ajudÃ¡-lo hoje?"
}
```

**CÃ³digos de Status:**
- `200` - Sucesso
- `400` - RequisiÃ§Ã£o invÃ¡lida
- `500` - Erro interno do servidor

## ğŸ”§ ConfiguraÃ§Ã£o de Provedores LLM

### Google Gemini
1. Acesse https://makersuite.google.com/app/apikey
2. Crie uma nova chave de API
3. Configure `GEMINI_API_KEY` e `LLM_PROVIDER=gemini`

**Nota**: Certifique-se de que a API key do Gemini estÃ¡ ativada e tem as permissÃµes necessÃ¡rias. Se houver erro 400, verifique se:
- A chave estÃ¡ correta
- O modelo `gemini-1.5-flash` estÃ¡ disponÃ­vel para sua conta
- A API Generative Language estÃ¡ habilitada no Google Cloud

## ğŸš¨ Tratamento de Erros

A aplicaÃ§Ã£o trata os seguintes cenÃ¡rios:

- **Timeout**: RequisiÃ§Ãµes que demoram mais que 30s
- **API Key invÃ¡lida**: Credenciais incorretas
- **Limite de rate**: Muitas requisiÃ§Ãµes
- **Erro de rede**: Problemas de conectividade
- **Resposta invÃ¡lida**: Formato inesperado da LLM

## ğŸ§ª Desenvolvimento

```bash
# VerificaÃ§Ã£o de tipos
npm run type-check

# Linting
npm run lint
```

## ğŸ“ LicenÃ§a

MIT